\documentclass{article}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Ying Qiao SID:21412301}
\rhead{Stat243 Fa12: Problem Set 4}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{listings}
\lstset{
	frame=tb,
	language=c,
	aboveskip=3mm,
	belowskip=3mm,
	basicstyle={\small\ttfamily},
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	prebreak=\textbackslash,
	showstringspaces=false,
	columns=fullflexible,
	upquote=true
	}

%% for inline R code: if the inline code is not correctly parsed, you will see a message
\newcommand{\rinline}[1]{SOMETHING WORNG WITH knitr}
%% begin.rcode setup, include=FALSE
% opts_chunk$set(fig.path='figure/latex-', cache.path='cache/latex-')
%% end.rcode


\begin{document}
\section*{Problem 1}

When adding the number 1 to 10000 copies of the number $1 \times 10^{-15}$, the answer mathematically is \newline
$1 + 1 \times 10^{-11} = 1.00000000001$. However, in order to explore computer-represented arithmetics, 
we want to use this as an example of the summation accuracy with numbers of very different magnitudes.

\subsubsection*{(a)}
Asumming we do not carry out the calculations in a dumb way, the best precision we can expect of our result
will be the machine \textit{double.eps}=\rinline{.Machine$double.eps}, i.e. 16 decimal places.

%% begin.rcode PS41a, cache=TRUE, results="hide", echo=FALSE
%%#### 1. add numbers of different sizes
rm(list = ls(all = TRUE)) 
options(digits = 22)
%% end.rcode

\subsubsection*{(b)}
As shown in the results below, using \textit{sum()} does not give the right answer (16 decimal places), but only
10 decimal places.

%% begin.rcode PS41b, cache=TRUE, results="markup"
%%# (b) 
x <- c(1, rep(1e-15, 10000))
sum(x) # No, 10 decimal places
%% end.rcode

\subsubsection*{(c)}
When using a \textit{for} loop to do the summation with $1$ as the first value, we do not get the right answer,
but only 11 decimal places. However, with $1$ as the last value during the summation, we can get the 16-decimal-place
right answer.

%% begin.rcode PS41c, cache=TRUE, results="markup"
%%# (c)
x <- c(1, rep(1e-15, 10000))
sum = 0
for (i in 1:length(x))  sum = sum + x[i]
sum # No, 11 decimal places

sum = 0
for (i in 2:length(x))  sum = sum + x[i]
sum + x[1] # YES, 16 decimal places
%% end.rcode

\subsubsection*{(d)}
Now, when considering 10000 copies of $1 \times 10^{-16}$, the results of repeating (b) and (c) are shown below.
Function \textit{sum()} gives about 11 decimal places, and the \textit{for} loop sum with leading $1$ gives underflown results.
However, \textit{for} loop sum with $1$ as the last value still gives the 16-decimal-place right answer.

%% begin.rcode PS41d, cache=TRUE, results="markup"
%%# (d) 1e-16 repeat b and c 
x <- c(1, rep(1e-16, 10000))
sum(x) # (b) No, 11 decimal places

sum = 0
for (i in 1:length(x))  sum = sum + x[i]
sum # (c) No, 0 decimal places, 1e-16 underflow double.eps

sum = 0
for (i in 2:length(x))  sum = sum + x[i]
sum + x[1] # (c) YES, 16 decimal places
%% end.rcode

\subsubsection*{(e)}
From the results in parts (b), (c) and (d), we can see that R's \textit{sum()} function does not sum
from left to right just using regular 16-decimal-place numbers. There is more work undergoing with the function.
See detailed exploration in part (g) given below.

\subsubsection*{(f)}
In order to only have 4 decimal places of accuracy in adding numbers of order of magnitude $10^{-16}$,
we need $10^{12}$ copies, which will take the storage space of $10^{12}\cdot 8[byte]/10^9 = $\rinline{1e12*8/1e9}Gb.

\subsubsection*{(g)}
The \textit{sum()} function in R calls a C function \textit{sum()} to do the summation.
%% begin.rcode PS41g, results="markup", echo = FALSE
base::sum
%% end.rcode
\begin{lstlisting}
void sum (int* x){
	long double s = 0.0;
	for (i = 0; i < n; i=i+1){
		s = s + x[i]
	}
	return s
}
\end{lstlisting}

In the C code, we can see that the "trick" is with the type definition of \textit{s} variable.
Using \textit{long double} in C, the machine implements \textit{s} with extended precision data type, i.e. \newline
80-bit number with sign(1b) + exp(15b) + int(1b) + frac(63b). This allows \textit{sum()} to give
almost accurate results when just summing left to right and convert down to regular double precision (64 bits).





\newpage
\section*{Problem 2}

We will ignore pivoting for the purpose of this problem when working out the number of calculations 
involved in the LU and Cholesky decomposition. 

\subsubsection*{(a)}
For an $n\times n$ invertible matrix, the number of flops involved in the forward reduction step of
the LU decomposition, which finds $L$ and $U$ excluding the calculations that change $b$ to $b^*$ is
\begin{lstlisting}
 LU Decomposition
 L{n-1}...L2*L1*A = U  //forward reduction deducted in class
 L = L{n-1}^-1...L2^-1*L1^-1  //no additional calculations
 b* = L{n-1}...L2*L1*b

 FLOPS	:	rows *( col{i} + 2*col{i+1}^n )  //zero-out ops are not calculated here
 L1			: (n-1)*(0 + 2*(n-1))
 L2			: (n-2)*(0 + 2*(n-2))
 ...
 L{n-1}	: 1*(0 + 1)
\end{lstlisting}
$LU(A_{nn}) = \sum_{i=1}^{n-1} 2(n-i)^2 = 2n^3/3 - n^2 + n/3$

\hbox{}
Comparing this to the number of caculations involved in the Cholesky decomposition, we have
\begin{lstlisting}
 Chol Decomposition
 A = UT*U //Positive definite (p.d.) matrix
 b* = UT^-1*b

 FLOPS		: a{ii} SQRT + col{i+1}^n DIV + (i-1)*col{i}^n MUL
 row 1		: 1 + (n-1) + 0
 row 2		: 1 + (n-2) + 1*(1 + n-2)
 row 3		: 1 + (n-3) + 2*(1 + n-3)
 ...
 row n-1	: 1 + 1 + (n-2)*(1 + 1)
 row n		: 1 + 0 + (n-1)*(1 + 0)
\end{lstlisting}
$Chol(A_{nn}) = \sum_{i=1}^n i(1 + n-i) = n^3/6 + n^2/2 + n/3$ 

\hbox{}
Now, we consider the additional computation flops involved in finding $b^*$
\newline
\hbox{}
\newline
$LU(b^*)  = \sum_{i=1}^n 2(i-1) = n^2 - n$
\newline
$Chol(b^*)  = \sum_{i=1}^n i  = n^2/2 + n/2$


\subsubsection*{(b)}
The additional flops involved in the backward elimination step that finds $x$ based on
$Ux=b^*$ is \newline
$LU(x) = Chol(x) = \sum_{i=1}^n i  = n^2/2 + n/2$

\subsubsection*{(c)}
The flops involved in part (a) and (b) when $b$ is acutally a matrix $B$ with $p$ columns are
simply $p$ times the regular vector-based flops, i.e.
\newline
$LU(B^*)  = p*LU(b^*) = pn^2 - pn$
\newline
$Chol(B^*) = p*Chol(b^*)  = pn^2/2 + pn/2$
\newline
\hbox{}
\newline
$LU(X) = p*LU(x) = pn^2/2 + pn/2$
\newline
$Chol(X) = p*Chol(x) = pn^2/2 + pn/2$

\subsubsection*{(d)}
Another way of doing this is to explicitly find the inverse, $A^{-1}$, and then multiply
$A^{-1}$ by the matrix $B$. The number of steps involved in using LU to find 
$V=A^{-1}  s.t.  LUV = I$ is
\begin{lstlisting}
 Matrix inversion and multiplication
 LUV = I // A = LU
 (1) find L,U; see results from (a) with LU(A_nn)
 (2) V = U^-1*(L^-1*I) = U^-1*(-L with diag(1))
\end{lstlisting}
$INV(A_{nn}) = LU(A_{nn}) + LU(X)_{p=n} = 7n^3/6 - n^2 + n/3$
\newline
$INV(B^*)  = 0 $ \hspace{18 pt} as $B$ is unchanged, no $B^*$ flops involved

\subsubsection*{(e)}
When we have $A^{-1}=V$ from part (d), the flops involved in calculating $VB$ is \newline
$INV(X) = pn^2$

\subsubsection*{(f)}
Finally we can compare the total flops involved in finding $A^{-1}B$ based on (c), (d) and (e).
\newline
$LU_O   = 2n^3/3 + (3p/2-1)n^2 + (1/3-p/2)n \sim O[(2n/3+3p/2)n^2]$
\newline
$INV_O  = 7n^3/6 + (p-1)n^2 + n/3 \sim O[(7n/6+p)n^2]$
\newline
$Chol_O = n^3/6 + (p+1/2)n^2 + (1/3+p)n \sim O[(n/6+p)n^2]$
\newline
\hbox{}
\newline
The comparison of which is better DOES depend on how big $p$ is. Roughly, when $p>>n$, $INV$ will
take fewer flops than $LU$, while $Chol$ is always better than the other two methods.

\subsubsection*{(g)}
Now, we are going to empirically test the flops results in R for calculating $A^{-1}B$ using
an arbitrary matrix $A=Z^TZ$ with $n\in (100,3000), p\in (1,100,3000)$.

%% begin.rcode PS42, cache=TRUE, results="markup", messages=FALSE
%%## (g) empirical comparison: calculate X = A^{-1}*B
options(digits = 4)
require(rbenchmark)
%%## (1) use LU: 				
LU <- function(A, B){ 
	solve(A, B)
}
%%## (2) use Inverse		
INV <- function(A, B){ 
	V <- solve(A)
	V %*% B
}
%%## (3) use Cholesky		
Chol <- function(A, B){ 
	U <- chol(A)
	backsolve(U, backsolve(U, B, transpose = TRUE))
}

n <- c(100, 3000) # really slow when n=3000, be careful
p <- c(1, 100, 3000)
for (i in 1:2){ 
	for (j in 1:3){
		nn <- n[i]
		pp <- p[j]
		LU_O   <- 2*nn^3/3 + (3*pp/2-1)*nn^2 + (1/3-pp/2)*nn
		INV_O  <- 7*nn^3/6 + (pp-1)*nn^2 + nn/3
		Chol_O <- nn^3/6 + (pp+1/2)*nn^2 + (1/3+pp)*nn
		cat("n =",nn,"and p =",pp,":\n")
		cat("LU O[f(n)]:  ", format(LU_O,  width = 16,justify = 'right'),"\n");
		cat("INV O[f(n)]: ", format(INV_O, width = 16,justify = 'right'),"\n");
		cat("Chol O[f(n)]:", format(Chol_O,width = 16,justify = 'right'),"\n"); 
		Z <- matrix(rnorm(nn^2), nn)
		A <- crossprod(Z) 
		B <- matrix(rnorm(nn*pp), nn) 
		cat("all.equal :",all.equal(LU(A,B), INV(A,B), Chol(A,B)), "\n")
		print(benchmark(LU(A,B), INV(A,B), Chol(A,B), replications = (2/i)^5)[1:6])
		cat("\n")
	}
}
%% end.rcode

\subsubsection*{(h)}
Now suppose we are going to have to do calculations with lots of new matrices, $B_j, j=1,2,...$
in the future. We have flops comparison, as below,
\newline
$LU(X_{future}) = LU(B^*) + LU(x) = 3pn^2/2 -pn/2 \sim O[(3n/2-1/2)pn]$
\newline
$INV(X_{future}) = INV(X) = pn^2 \sim O[n\cdot pn]$
\newline
As shown above, there are advantages to pre-compute $A^{-1}$ with large $p$.


\newpage
\section*{Problem 3}
To compute the generalized least squares estimator, 
$\hat\beta = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}Y$
for $X n\times p, \Sigma n\times n; n\sim 1000, p\sim 100$, we will
do this in an efficient way with the following pseudo-code:

\begin{lstlisting}
// S = W * W'
W = transpose(cholesky(S)); 
//          X' * S^-1 * X * beta = X' * S^-1 * Y
//   X' * (W * W')^-1 * X * beta = X' * (W * W')^-1 * Y
//  X' * W'^-1 * W^-1 * X * beta = X' * W'^-1 * W^-1 * Y
//             X_s' * X_s * beta = X_s' * Y_s			
X_s = W^(-1) * X;
Y_s = W^(-1) * Y;
// Reduced from GLS to OLS:
X_s.qr = qr(X_s); 
// Q: orthogonal; R: upper triangular
Q = qr.Q(X_s.qr);  
R = qr.R(X_s.qr); 
//             X_s' * X_s * beta = X_s' * Y_s			
//        R' * Q' * Q * R * beta = R' * Q' * Y
//                      R * beta = Q' * Y
beta = R^(-1) * transpose(Q) * Y
\end{lstlisting}

In R, my function \textit{gls()} is developed to do this computation.
%% begin.rcode PS43, cache=TRUE, results="markup"
%%#### 3. GLS estimator
n <- 2000
p <- 200

Z <- matrix(abs(rnorm(n^2)), n)
S <- crossprod(Z)/max(Z)
X <- matrix(rnorm(n*p)*10, n)
Y <- matrix(rnorm(n)*100, n)

gls <- function(X,S,Y){
	W <- t(chol(S)) ## S = W * W'
	X_s <- solve(W, X) ## X* = W^(-1) * X
	Y_s <- solve(W, Y)
	X_s.qr <- qr(X_s)
	Q = qr.Q(X_s.qr)
	R = qr.R(X_s.qr)
	beta <- backsolve(R, crossprod(Q,Y))	
	return(beta) ## efficient way
}

beta_hat <- function(X,S,Y){
	Xt <- t(X)
	Sinv <- solve(S)
	beta <- solve(Xt %*% Sinv %*% X) %*% Xt %*% Sinv %*% Y
	return(beta) ## naive way
}

benchmark(gls(X,S,Y), beta_hat(X,S,Y), replications = 5)[1:6]

%% end.rcode

We can see that there is 2X speed gain with function \textit{gls()}.

\newpage
\section*{Problem 4}
Here, we want to explore whether integer calculations in R are faster than
floating point calculations.

%% begin.rcode PS44, cache=TRUE, results="markup", messages=FALSE
%%#### 4. compare integer and floating point calculations
require(rbenchmark)
options(digits = 4)
### comparison: Integer vs. Double
xi <- as.integer(rnorm(1000000)) # 1e6 samples
xf <- rnorm(1000000)
benchmark(xi[100:10000], xf[100:10000], replications = 100)[1:6] #subsetting
benchmark(sum(xi), sum(xf), replications = 100)[1:6] #arithmetic: sum
benchmark(crossprod(xi), crossprod(xf), replications = 100)[1:6] #arithmetic: mul
%% end.rcode
As shown above in the results, this claim is not necessary true. It seems that 
integer arithmetics are slower while subsetting vectors is almost the same as compared to
floating points.

\newpage
\section*{Problem 5}
We want to empirically explore the condition number of the eigen-decomposition.

%% begin.rcode PS45, cache=TRUE, results="markup", tidy=FALSE, dev='cairo_pdf', fig.width=8, fig.height=10, fig.align="center"
%%#### 5. Condition number of eigen decomposition
norm2 <- function(x) sqrt(sum(x^2)) # aux function calc norm2 of vecs
%%### Create a set of eigenvectors
n <- 100
Z <- matrix(rnorm(n^2), n)
A <- crossprod(Z)
GAMMA <- eigen(A)$vec #eigen vectors created from a random p.d. matrix
%%### Explore positive eigenvalues of different magnitudes
t <- .Machine$double.digits #computer presicion bits
errors <- rep(as.numeric(NA), t) #relative error
magval <- rep(as.numeric(NA), t) #eigen value magnitude
condno <- rep(as.numeric(NA), t) #condition number
for (i in 1:t){
%%    ## get a random set of indices to enlarge with condition number
    ids <- sample(1:n, n/4) 
    if (i == 1){ # i=1, with all eigen values equal
        lambda <- rep(mean(abs(rnorm(n))), n) 
    }
    else{
        lambda <- abs(rnorm(n))
    }
%%    ## generate eigen values with certain condition number
    lambda[ids] <- lambda[ids]*(2^(i-1)) 
    magval[i] <- max(lambda)
    condno[i] <- magval[i]/min(lambda)
	
    LAMBDA <- diag(lambda)
    testA <- GAMMA %*% LAMBDA %*% t(GAMMA) # create p.d. matrix
    errors[i] <- norm2(eigen(testA)$val - lambda)/norm2(lambda)
    if (length(which(eigen(testA)$val < 0)) > 0){ # not p.d.
        cat("Failed at condition number", condno[i], 
            "with relative error of", errors[i], "\n")
        par(mfrow=c(2,1),cex= 1, cex.main= 1)
        plot(log10(condno), errors, xlim=c(0, 17), ylim=c(0, 1.6),
             main="Relative error in estimated eigenvalues vs. condition number")
        plot(log10(magval), errors, xlim=c(0, 17), ylim=c(0, 1.6),
             main="Relative error in estimated eigenvalues vs. magnitude of eigenvalues")
        break
    }
}
%% end.rcode

As shown in the results above, the test matrix fails to be numerically positive definite with
condition number around $10^{17}$ and estimated eigenvalue relative error around $1.2$.
The plots show that the error saturates pretty quickly at the smaller condition numbers, 
but only when the condition number overflows, the matrix will no longer be positive definite.

\newpage
\section*{Problem 6}
In order to  see the representation of decimal numbers in base-2 computer bits, both R and C codes are
written to see the rounding or "garbage digits" phenomenon.
In the R code, we can see "rounding" with $x \sim 10^{-17}$, 
when $x$ get multiplied by $2^{56}$ to be converted into integers.

%% begin.rcode PS46, cache=TRUE, results="markup", tidy = FALSE
%%#### 6. "Garbage" digits
options(digits = 22)
x <- c(0.12345678123456780000, 0.12345678123456781000,
       0.12345678123456782000, 0.12345678123456783000,
       0.12345678123456784000, 0.12345678123456785000,
       0.12345678123456786000, 0.12345678123456787000,
       0.12345678123456788000, 0.12345678123456789000)
x
x*2^56 # see longdouble.c (consistent, last bit is garbage)

y <- c(0.0000000012345678123456780000, 0.0000000012345678123456781000,
       0.0000000012345678123456782000, 0.0000000012345678123456783000,
       0.0000000012345678123456784000, 0.0000000012345678123456785000,
       0.0000000012345678123456786000, 0.0000000012345678123456787000,
       0.0000000012345678123456788000, 0.0000000012345678123456789000)
y
y*2^82 # see longdouble.c (minor inconsistency, last bit is also garbage)


%% end.rcode

We can see that there really is "rounding" with these numbers when represented using
base-2 computer numbers. The C code uses \textit{long double} type to represent floating points in 80 bits, 
which gives us the references when verifying the R code results.

\begin{lstlisting}
#include <stdio.h>
#include <math.h>
int main (void)
{
	long double a[] = {0.12345678123456780000,0.12345678123456781000,0.12345678123456782000,
			   0.12345678123456783000,0.12345678123456784000,0.12345678123456785000,
			   0.12345678123456786000,0.12345678123456787000,0.12345678123456788000,
			   0.12345678123456789000};
	long double b[] = {0.0000000012345678123456780000,0.0000000012345678123456781000,
			   0.0000000012345678123456782000,0.0000000012345678123456783000,
			   0.0000000012345678123456784000,0.0000000012345678123456785000,
			   0.0000000012345678123456786000,0.0000000012345678123456787000,
			   0.0000000012345678123456788000,0.0000000012345678123456789000};
	long double ax2[10];
	long double by2[10];
	long double x2 = powl(2, 56);
	long double y2 = powl(2, 82);
	int i=0;
	printf("x2 = %Lf\n", x2);
	for (i=0; i<10; i++){
	  ax2[i] = a[i]*x2;
	  printf("long_d a = %.60Lf\n", a[i]);
	  printf("integera = %Lf\n", ax2[i]);
	}
	printf("y2 = %Lf\n", y2);
	for (i=0; i<10; i++){
	  by2[i] = b[i]*y2;
	  printf("long_d b = %.90Lf\n", b[i]);
	  printf("integerb = %Lf\n", by2[i]);
	}
	return 0;
}
\end{lstlisting}
\hbox{}
\input{"./C4.6.tex"}

We can see that C code results verifies our conclusion that the last bit is "garbage bit" here.

\end{document}
